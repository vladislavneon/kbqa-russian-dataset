{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb6c75e046e49eba90519b837e768a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('labels_new_v3.txt', 'r') as inf:\n",
    "    labels = defaultdict(list)\n",
    "    for line in tqdm(inf):\n",
    "        qid, label = line.strip('\\n').split(':', 1)\n",
    "        labels[qid].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc895e5329934bc2b6d287fd7234aaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4114595), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tqdm(labels.items()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, job_name=None):\n",
    "        self.job_name = job_name\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time()\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        end_time = time()\n",
    "        passed_time = end_time - self.start_time\n",
    "        line = f'{self.job_name} executed in {passed_time:.3f} s.' if self.job_name else f'{passed_time:.3f} s.'\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply analyzer to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "simple_tokenizer = CountVectorizer(lowercase=False, token_pattern='\\w+').build_analyzer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return(' '.join(simple_tokenizer(text)))\n",
    "\n",
    "def analyzer(text):\n",
    "    return(' '.join(simple_analyzer(''.join(mystem.lemmatize(text)))))\n",
    "\n",
    "def analyzer2(text):\n",
    "    return(''.join(mystem.lemmatize(' '.join(simple_analyzer(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996b5b05ce9b4acbbee17588d1ded978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with open('labels_new_v3_test.txt', 'r') as inf, open('labels_mystem_test.txt', 'w') as ouf:\n",
    "#     for line in tqdm(inf):\n",
    "#         qid, label = line.strip('\\n').split(':', 1)\n",
    "#         ouf.write(qid + ':' + analyzer(label) + '\\n')\n",
    " \n",
    "# create file with only tokenized labels\n",
    "with open('labels_raw.txt', 'r') as inf, open('labels_token.txt', 'w') as ouf:\n",
    "    for line in tqdm(inf):\n",
    "        qid, label = line.strip('\\n').split(':', 1)\n",
    "        ouf.write(qid + ':' + tokenizer(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create elastic search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(key, data, index):\n",
    "    es_document = {'qid': key, 'label': data}\n",
    "    es_action = {\n",
    "        \"_index\": index,\n",
    "        \"_source\": es_document\n",
    "    }\n",
    "    return es_action\n",
    "\n",
    "documents_file = 'labels_token.txt'\n",
    "\n",
    "def label_list_documents_generator():\n",
    "    with open(documents_file, 'r') as file:\n",
    "        prev_qid = None\n",
    "        cur_label_list = []\n",
    "        for line in tqdm(file):\n",
    "            qid, label = line.split(':', 1)\n",
    "            if qid == prev_qid:\n",
    "                cur_label_list.append(label.strip('\\n'))\n",
    "            else:\n",
    "                if len(cur_label_list) > 0:\n",
    "                    yield create_es_action(prev_qid, cur_label_list, 'label_list')\n",
    "\n",
    "                prev_qid = qid\n",
    "                cur_label_list = [label.strip('\\n')]\n",
    "        \n",
    "        if len(cur_label_list) > 0:\n",
    "            yield create_es_action(prev_qid, cur_label_list, 'label_list')\n",
    "            \n",
    "def label_single_documents_generator():\n",
    "    with open(documents_file, 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            qid, label = line.split(':', 1)\n",
    "            yield create_es_action(qid, label.strip('\\n'), 'label_single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'label_single'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"text\",\n",
    "                'analyzer': 'snowball_stemmer'\n",
    "#                 \"fields\": {\n",
    "#                     \"shingle\": {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"analyzer\": \"shingle_analyzer\"\n",
    "#                     }\n",
    "#                 }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\" : {\n",
    "            \"analyzer\" : {\n",
    "                \"snowball_stemmer\" : {\n",
    "                    \"tokenizer\" : \"whitespace\",\n",
    "                    \"filter\" : ['lowercase', \"snow_stem\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"snow_stem\" : {\n",
    "                    \"type\" : \"snowball\",\n",
    "                    \"language\" : \"Russian\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "#         \"analysis\": {\n",
    "#             \"analyzer\": {\n",
    "#                 \"shingle_analyzer\": {\n",
    "#                     \"tokenizer\": \"standard\",\n",
    "#                     \"filter\": [\n",
    "#                         \"custom_shingle\"\n",
    "#                     ]\n",
    "#                 }\n",
    "#             },\n",
    "#             \"filter\": {\n",
    "#                 \"custom_shingle\": {\n",
    "#                     \"type\": \"shingle\",\n",
    "#                     \"min_shingle_size\": \"2\",\n",
    "#                     \"max_shingle_size\": \"3\"\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.delete(index='label_list')\n",
    "es.indices.create(index='label_list', body=settings)\n",
    "\n",
    "es.indices.delete(index='label_single')\n",
    "es.indices.create(index='label_single', body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccc1f6d1a3445efa4d601983be8d65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ok, result in parallel_bulk(es, label_single_documents_generator(), queue_size=8, thread_count=8, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic query builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('russian_stopwords.txt', 'r') as inf:\n",
    "    stopwords = []\n",
    "    for line in inf:\n",
    "        stopwords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos_tags = [\n",
    "    'ADVPRO',\n",
    "    'APRO',\n",
    "    'CONJ',\n",
    "    'INTJ',\n",
    "    'PART',\n",
    "    'PR',\n",
    "    'SPRO',\n",
    "    'V',\n",
    "    'ADV'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TokenFilter:\n",
    "#     def __init__(self, stopwords, stop_pos_tags):\n",
    "#         self.stopwords = set(stopwords)\n",
    "#         self.stop_pos_tags = set(stop_pos_tags)\n",
    "        \n",
    "#     def is_good(self, token):\n",
    "#         if self.has_first_capital(token):\n",
    "#             return True\n",
    "#         if self.pass_stopwords(token) and \\\n",
    "#            self.pass_length(token):\n",
    "#             return True\n",
    "#         return False\n",
    "        \n",
    "#     def pass_stopwords(self, token):\n",
    "#         if token in self.stopwords:\n",
    "#             return False\n",
    "#         return True\n",
    "    \n",
    "#     def pass_length(self, token):\n",
    "#         if len(token) < 2:\n",
    "#             return False\n",
    "#         return True\n",
    "    \n",
    "#     def has_first_capital(self, token):\n",
    "#         if token[0].isupper():\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "class Morpher:\n",
    "    def __init__(self):\n",
    "        self.stop_pos_tags = set([\n",
    "            'ADVPRO',\n",
    "            'APRO',\n",
    "            'CONJ',\n",
    "            'INTJ',\n",
    "            'PART',\n",
    "            'PR',\n",
    "            'SPRO',\n",
    "            'V',\n",
    "            'ADV'\n",
    "        ])\n",
    "        self.mystem = Mystem()\n",
    "        \n",
    "    def analyze(self, text):\n",
    "        return self.mystem.analyze(text)\n",
    "    \n",
    "    def approve_tag(self, tag):\n",
    "        if tag in self.stop_pos_tags:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, text, tokenizer, morpher):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.morpher = morpher\n",
    "        self.get_tokens()\n",
    "        self.get_filtered_tokens()\n",
    "        self.get_token_ngrams()\n",
    "        self.get_capital_pairs()\n",
    "        \n",
    "    def get_tokens(self):\n",
    "        self.tokens = self.tokenizer(self.text).split()\n",
    "        return self.tokens\n",
    "    \n",
    "    def get_filtered_tokens(self):\n",
    "        self.filtered_tokens = []\n",
    "        analysis = self.morpher.analyze(' '.join(self.tokens))\n",
    "        for i, ta in enumerate(analysis):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if 'analysis' in ta:\n",
    "                if ta['text'][0].isupper() and len(ta['text']) > 1:\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "                    continue\n",
    "                pos_tag = ta['analysis'][0]['gr'].split(',', 1)[0].split('=', 1)[0]\n",
    "                if self.morpher.approve_tag(pos_tag):\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "        return self.filtered_tokens\n",
    "    \n",
    "    def get_token_ngrams(self, n=3):\n",
    "        self.token_ngrams = []\n",
    "        for i in range(len(self.tokens) - (n - 1)):\n",
    "            self.token_ngrams.append(' '.join(self.tokens[i:(i + n)]))\n",
    "        return self.token_ngrams\n",
    "    \n",
    "    def get_capital_pairs(self):\n",
    "        self.capital_pairs = []\n",
    "        for i in range(1, len(self.tokens) - 1):\n",
    "            if self.tokens[i][0].isupper() and \\\n",
    "               len(self.tokens[i]) > 1 and \\\n",
    "               self.tokens[i + 1][0].isupper() and \\\n",
    "               len(self.tokens[i + 1]) > 1:\n",
    "                self.capital_pairs.append(' '.join(self.tokens[i:(i + 2)]))\n",
    "        return self.capital_pairs\n",
    "    \n",
    "    def build_match_query(self, query, fuzziness='AUTO'):\n",
    "        return  { \n",
    "                    'match': {\n",
    "                        'label': {\n",
    "                            'query': query,\n",
    "                            \"fuzziness\": fuzziness,\n",
    "                            \"prefix_length\": 1,\n",
    "                            'fuzzy_transpositions': False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    def build_phrase_query(self, query):\n",
    "        return  { \n",
    "                    'match_phrase': {\n",
    "                        'label': query\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    def get_phrase_queries(self):\n",
    "        qs = []\n",
    "        for ng in self.token_ngrams:\n",
    "            qs.append(self.build_phrase_query(ng))\n",
    "        for cp in self.capital_pairs:\n",
    "            qs.append(self.build_phrase_query(cp))\n",
    "        return qs\n",
    "    \n",
    "    def get_fulltext_queries(self):\n",
    "        return [self.build_match_query(' '.join(self.filtered_tokens))]\n",
    "    \n",
    "    def get_single_capital_queries(self):\n",
    "        qs = []\n",
    "        for token in self.filtered_tokens:\n",
    "            if token[0].isupper() and len(token) > 2:\n",
    "                qs.append(self.build_phrase_query(token))\n",
    "        return qs\n",
    "    \n",
    "    def get_partial_queries(self):\n",
    "        qs = []\n",
    "        n = len(self.filtered_tokens)\n",
    "        for i in range(0, n, 2):\n",
    "            subtext = ' '.join(self.filtered_tokens[i:(min(i + 3, n))])\n",
    "            qs.append(self.build_match_query(subtext, fuzziness=0))\n",
    "        return qs\n",
    "        \n",
    "    def build_es_query(self, queries):\n",
    "        return  {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'should': queries\n",
    "                        }\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.551 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'match_phrase': {'label': 'О какой битве'}},\n",
       " {'match_phrase': {'label': 'какой битве в'}},\n",
       " {'match_phrase': {'label': 'битве в российской'}},\n",
       " {'match_phrase': {'label': 'в российской истории'}},\n",
       " {'match_phrase': {'label': 'российской истории говорил'}},\n",
       " {'match_phrase': {'label': 'истории говорил русский'}},\n",
       " {'match_phrase': {'label': 'говорил русский поэт'}},\n",
       " {'match_phrase': {'label': 'русский поэт А'}},\n",
       " {'match_phrase': {'label': 'поэт А Пушкин'}}]"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrph = Morpher()\n",
    "with Timer():\n",
    "    q = Query('О какой битве в российской истории говорил русский поэт А . Пушкин?', tokenizer=tokenizer, morpher=mrph)\n",
    "q.get_phrase_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'bool': {'should': [{'match_phrase': {'label': 'О какой битве'}},\n",
       "    {'match_phrase': {'label': 'какой битве в'}},\n",
       "    {'match_phrase': {'label': 'битве в российской'}},\n",
       "    {'match_phrase': {'label': 'в российской истории'}},\n",
       "    {'match_phrase': {'label': 'российской истории говорил'}},\n",
       "    {'match_phrase': {'label': 'истории говорил русский'}},\n",
       "    {'match_phrase': {'label': 'говорил русский поэт'}},\n",
       "    {'match_phrase': {'label': 'русский поэт А'}},\n",
       "    {'match_phrase': {'label': 'поэт А Пушкин'}}]}}}"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.build_es_query(q.get_phrase_queries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['профессиональное', 'прозвище', 'Максима', 'Галкина']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': {'value': 3151, 'relation': 'eq'},\n",
       " 'max_score': 15.187308,\n",
       " 'hits': [{'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'Z9rupmwBg_1FN09H97FX',\n",
       "   '_score': 15.187308,\n",
       "   '_source': {'qid': 'Q49614', 'label': 'прозвище'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'PhnxpmwBg_1FN09H_iAA',\n",
       "   '_score': 12.729599,\n",
       "   '_source': {'qid': 'Q16593872', 'label': 'Галкина Галина Анатольевна'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'PxnxpmwBg_1FN09H_iAA',\n",
       "   '_score': 12.729599,\n",
       "   '_source': {'qid': 'Q16593872', 'label': 'Галина Анатольевна Галкина'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '3ejvpmwBg_1FN09HueDA',\n",
       "   '_score': 12.183937,\n",
       "   '_source': {'qid': 'Q1371427', 'label': 'национальное прозвище'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '3ujvpmwBg_1FN09HueDA',\n",
       "   '_score': 12.183937,\n",
       "   '_source': {'qid': 'Q1371427', 'label': 'национальные прозвища'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'VN3vpmwBg_1FN09HFSTG',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q165740', 'label': 'Максима'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'CfnwpmwBg_1FN09HgJ6F',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q4276243', 'label': 'Максимов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'WujvpmwBg_1FN09HuuYN',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q1374354', 'label': 'максима'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'CgLwpmwBg_1FN09H4Qno',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q6740334', 'label': 'Максимов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '_fTwpmwBg_1FN09HR87n',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q3853105', 'label': 'Максима'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'IvPwpmwBg_1FN09HOY1k',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q3454751', 'label': 'Максимы'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'xuHvpmwBg_1FN09HUcvj',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q520952', 'label': 'максима'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'yx7ypmwBg_1FN09HQW_b',\n",
       "   '_score': 11.93012,\n",
       "   '_source': {'qid': 'Q20000642', 'label': 'Максимо'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'SejvpmwBg_1FN09HsBbA',\n",
       "   '_score': 11.601175,\n",
       "   '_source': {'qid': 'Q1218318', 'label': 'Профессионалы'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'jt7vpmwBg_1FN09HKI9O',\n",
       "   '_score': 11.323534,\n",
       "   '_source': {'qid': 'Q245445',\n",
       "    'label': 'Кто хочет стать Максимом Галкиным'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'QSXypmwBg_1FN09HmENL',\n",
       "   '_score': 11.323534,\n",
       "   '_source': {'qid': 'Q32360836',\n",
       "    'label': 'Кто хочет стать Максимом Галкиным'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'WgfxpmwBg_1FN09HJbO0',\n",
       "   '_score': 11.168377,\n",
       "   '_source': {'qid': 'Q11336312', 'label': 'профессиональный бейсболист'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'ANrupmwBg_1FN09H8kFl',\n",
       "   '_score': 11.168377,\n",
       "   '_source': {'qid': 'Q28640', 'label': 'профессиональная деятельность'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'vdrupmwBg_1FN09H8kvO',\n",
       "   '_score': 11.168377,\n",
       "   '_source': {'qid': 'Q31022', 'label': 'Профессиональная этика'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'Nt3vpmwBg_1FN09HGVwR',\n",
       "   '_score': 11.168377,\n",
       "   '_source': {'qid': 'Q178790', 'label': 'профессиональный союз'}}]}"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = tokenizer('Кого сыграл Жан Рено в фильме \"Ее звали Никита\"?')\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         \"fuzzy\" : {\n",
    "#             \"labels\" : {\n",
    "#                 \"value\": \"домашняя кошка\",\n",
    "#                 \"fuzziness\": 2,\n",
    "#                 \"prefix_length\": 0,\n",
    "#                 \"max_expansions\": 100,\n",
    "#                 'transpositions': False\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         'match': {\n",
    "#             'label': {\n",
    "#                 'query': query,\n",
    "#                 \"fuzziness\": 'AUTO',\n",
    "#                 \"prefix_length\": 1,\n",
    "#                 'fuzzy_transpositions': False\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         'match_phrase': {\n",
    "#             'label': query\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "doc = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "#                 {\n",
    "#                     'match_phrase': {\n",
    "#                         'label': 'Жан Рено'\n",
    "#                     }\n",
    "#                 },\n",
    "#                 {\n",
    "#                     'match_phrase': {\n",
    "#                         'label': 'Ее звали Никита'\n",
    "#                     }\n",
    "#                 },\n",
    "                { \n",
    "                    'match': {\n",
    "                        'label': {\n",
    "                            'query': 'Пушкин',\n",
    "                            \"fuzziness\": 'AUTO:7,10',\n",
    "                            \"prefix_length\": 1,\n",
    "                            'fuzzy_transpositions': False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         'match_all': {}\n",
    "#     }\n",
    "# }\n",
    "q = Query('Какое профессиональное прозвище у Максима Галкина?', tokenizer=tokenizer, morpher=mrph)\n",
    "print(q.filtered_tokens)\n",
    "doc = q.build_es_query(q.get_fulltext_queries())\n",
    "\n",
    "res = es.search(index='label_single', body=doc, size=20)\n",
    "res['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matcher:\n",
    "    def __init__(self, es_instance):\n",
    "        self.es = es_instance\n",
    "        \n",
    "    def _smooth_score(self, score, d=5):\n",
    "        score = int(score)\n",
    "        while score % d != 0:\n",
    "            score += 1\n",
    "        return score\n",
    "    \n",
    "    def _parse_id(self, qid):\n",
    "        return int(qid[1:])\n",
    "    \n",
    "    def _sorting_key(self, match):\n",
    "        score = self._smooth_score(match['score'])\n",
    "        qid = self._parse_id(match['qid'])\n",
    "        return score, -qid\n",
    "    \n",
    "    def get_names_and_descriptions(self, qids):\n",
    "        qids_list = '|'.join(qids)\n",
    "        wikiapi_query = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={qids_list}&languages=ru&props=labels|descriptions'\n",
    "        resp = requests.get(wikiapi_query).json()\n",
    "        result = {}\n",
    "        for qid in qids:\n",
    "            cur_data = resp['entities'][qid]\n",
    "            name = None\n",
    "            if 'ru' in cur_data['labels']:\n",
    "                name = cur_data['labels']['ru']['value']\n",
    "            description = None\n",
    "            if 'ru' in cur_data['descriptions']:\n",
    "                description = cur_data['descriptions']['ru']['value']\n",
    "            result[qid] = {'name': name, 'description': description}\n",
    "        return result\n",
    "    \n",
    "    def get_wikipedia_pageviews(self, titles_dict):\n",
    "        if not titles_dict:\n",
    "            return\n",
    "        titles_list = list(map(lambda s: s.replace(' ', '_'), titles_dict.keys()))\n",
    "        for i in range(len(titles_list)):\n",
    "            titles = '|'.join(titles_list)\n",
    "            wikiapi_query = f'https://ru.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&pvipdays=30&titles={titles}'\n",
    "            resp = requests.get(wikiapi_query).json()\n",
    "            if 'batchcomplete' in resp:\n",
    "                break\n",
    "#         print(resp)\n",
    "        pages_data = (resp['query']['pages'])\n",
    "#         print(len(titles_list))\n",
    "#         print(len(pages_data.values()))\n",
    "        \n",
    "        for entry in pages_data.values():\n",
    "            view_stats = entry['pageviews']\n",
    "            views = sum(filter(None, view_stats.values()))\n",
    "            cur_title = entry['title']\n",
    "            titles_dict[cur_title] = views\n",
    "    \n",
    "    def get_wikipedia_pages(self, qids):\n",
    "        qids_list = '|'.join(qids)\n",
    "        wikiapi_query = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={qids_list}&sitefilter=ruwiki&props=sitelinks'\n",
    "        resp = requests.get(wikiapi_query).json()\n",
    "        result = {}\n",
    "        views = {}\n",
    "        for qid in qids:\n",
    "            cur_data = resp['entities'][qid]\n",
    "            wiki_title = None\n",
    "            if 'ruwiki' in cur_data['sitelinks']:\n",
    "                wiki_title = cur_data['sitelinks']['ruwiki']['title']\n",
    "                views[wiki_title] = 0\n",
    "            result[qid] = {'ruwiki': wiki_title}\n",
    "            \n",
    "        self.get_wikipedia_pageviews(views)\n",
    "        for qid in qids:\n",
    "            cur_title = result[qid]['ruwiki']\n",
    "            if cur_title is not None:\n",
    "                result[qid]['views'] = views[cur_title]\n",
    "            else:\n",
    "                result[qid]['views'] = 0\n",
    "        return result\n",
    "    \n",
    "#     def get_wikipedia_pageviews(self, wiki_title):\n",
    "#         wiki_title = wiki_title.replace(' ', '_')\n",
    "#         wikiapi_query = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ru.wikipedia/all-access/all-agents/{wiki_title}/monthly/2019010100/2019013100'\n",
    "#         resp = requests.get(wikiapi_query).json()\n",
    "#         return resp['items'][0]['views']\n",
    "\n",
    "    def _apply_ranking(self, matches, relative_rate=0.9):\n",
    "        if len(matches) == 0:\n",
    "            return matches\n",
    "        \n",
    "        matches = sorted(matches, key=lambda m: m['score'], reverse=True)      \n",
    "        \n",
    "        cur_max_pos = 0\n",
    "        cur_pos = 1\n",
    "        while cur_pos < len(matches):\n",
    "            cur_max_score = matches[cur_max_pos]['score']\n",
    "            while cur_pos < len(matches) and matches[cur_pos]['score'] >= relative_rate * matches[cur_pos - 1]['score']:\n",
    "                cur_pos += 1\n",
    "            matches[cur_max_pos:cur_pos] = sorted(matches[cur_max_pos:cur_pos], key=lambda m: m['views'], reverse=True)\n",
    "            cur_max_pos = cur_pos\n",
    "            cur_pos = cur_max_pos + 1\n",
    "            \n",
    "        return matches\n",
    "        \n",
    "    def get_query_matches(self, query, n_matches=30):\n",
    "        es_result = es.search(index='label_single', body=query, size=n_matches)['hits']\n",
    "        matches_dict = {}\n",
    "        for entry in es_result['hits']:\n",
    "            qid = entry['_source']['qid']\n",
    "            if qid not in matches_dict:\n",
    "                matches_dict[qid] = entry['_source']\n",
    "                matches_dict[qid]['score'] = entry['_score']\n",
    "        \n",
    "        names_and_descriptions = self.get_names_and_descriptions(matches_dict.keys())\n",
    "        for qid, nds in names_and_descriptions.items():\n",
    "            matches_dict[qid].update(nds)\n",
    "            \n",
    "        wikipedia_pages = self.get_wikipedia_pages(matches_dict.keys())\n",
    "        for qid, wps in wikipedia_pages.items():\n",
    "            matches_dict[qid].update(wps)\n",
    "            \n",
    "#         for qid in tqdm(matches_dict):\n",
    "#             match = matches_dict[qid]\n",
    "#             match['views'] = 0\n",
    "#             if match['ruwiki'] is not None:\n",
    "#                 match['views'] = self.get_wikipedia_pageviews(match['ruwiki'])\n",
    "            \n",
    "#         matches = sorted(matches_dict.values(), key=self._sorting_key, reverse=True)\n",
    "        matches = self._apply_ranking(list(matches_dict.values()))\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtc = Matcher(es_instance=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'bool': {'should': [{'match_phrase': {'label': 'Какое профессиональное прозвище'}},\n",
       "    {'match_phrase': {'label': 'профессиональное прозвище у'}},\n",
       "    {'match_phrase': {'label': 'прозвище у Максима'}},\n",
       "    {'match_phrase': {'label': 'у Максима Галкина'}},\n",
       "    {'match_phrase': {'label': 'Максима Галкина'}}]}}}"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.build_es_query(q.get_single_capital_queries()[0])\n",
    "q.build_es_query(q.get_phrase_queries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qid': 'Q32360836',\n",
       "  'label': 'Кто хочет стать Максимом Галкиным',\n",
       "  'score': 12.354108,\n",
       "  'name': 'Кто хочет стать Максимом Галкиным?',\n",
       "  'description': None,\n",
       "  'ruwiki': 'Кто хочет стать Максимом Галкиным?',\n",
       "  'views': 324},\n",
       " {'qid': 'Q245445',\n",
       "  'label': 'Кто хочет стать Максимом Галкиным',\n",
       "  'score': 12.354108,\n",
       "  'name': 'Кто хочет стать Максимом Галкиным?',\n",
       "  'description': None,\n",
       "  'ruwiki': None,\n",
       "  'views': 0}]"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mtc.get_query_matches(q.build_es_query(q.get_fulltext_queries()))\n",
    "# mtc.get_query_matches(q.build_es_query(q.get_single_capital_queries()[0]))\n",
    "mtc.get_query_matches(q.build_es_query(q.get_phrase_queries()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Suggester:\n",
    "    def __init__(self, matcher, tokenizer, morpher):\n",
    "        self.matcher = matcher\n",
    "        self.tokenizer = tokenizer\n",
    "        self.morpher = morpher\n",
    "        \n",
    "    def _build_query(self, text):\n",
    "        return Query(text=text, tokenizer=self.tokenizer, morpher=self.morpher)\n",
    "    \n",
    "    def _get_matches(self, q, query_type):\n",
    "        if query_type == 'fulltext':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_fulltext_queries()), n_matches=20)\n",
    "        if query_type == 'phrase':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_phrase_queries()), n_matches=8)\n",
    "        if query_type == 'single':\n",
    "            single_queries = q.get_single_capital_queries()\n",
    "            return [self.matcher.get_query_matches(q.build_es_query(single_query), n_matches=10)\n",
    "                   for single_query in single_queries]\n",
    "        \n",
    "    def _select_matches(self, q, min_total=8, f=5, p=4, s=1):\n",
    "        matches = []\n",
    "        matches_qids = set()\n",
    "        \n",
    "#         print('# Phrase')\n",
    "        phrase_matches = self._get_matches(q, query_type='phrase')\n",
    "        if len(phrase_matches) > p:\n",
    "            phrase_matches = phrase_matches[:p]\n",
    "        for match in phrase_matches:\n",
    "            matches_qids.add(match['qid'])\n",
    "            match['source'] = 'phrase'\n",
    "#             print(match)\n",
    "        \n",
    "#         print('# Singles')\n",
    "        single_matches = self._get_matches(q, query_type='single')\n",
    "        single_matches_result = []\n",
    "        for single_match in single_matches:\n",
    "            top_match = single_match[0]\n",
    "            if top_match['qid'] not in matches_qids:\n",
    "                matches_qids.add(top_match['qid'])\n",
    "                top_match['source'] = 'single'\n",
    "                single_matches_result.append(top_match)\n",
    "#                 print(top_match)\n",
    "        single_matches_result = sorted(single_matches_result, key=lambda m: m['views'], reverse=True)\n",
    "        \n",
    "#         print('# Fulltext')\n",
    "        fulltext_matches = self._get_matches(q, query_type='fulltext')\n",
    "        fulltext_matches_result = []\n",
    "        f_cnt = 0\n",
    "        for match in fulltext_matches:\n",
    "            if match['qid'] not in matches_qids:\n",
    "                f_cnt += 1\n",
    "                matches_qids.add(match['qid'])\n",
    "                match['source'] = 'fulltext'\n",
    "                fulltext_matches_result.append(match)\n",
    "#                 print(match)\n",
    "            if len(matches_qids) >= min_total and f_cnt >= f:\n",
    "                break\n",
    "        fulltext_matches_result = sorted(fulltext_matches_result, key=lambda m: m['views'], reverse=True)\n",
    "                \n",
    "        matches.extend(phrase_matches)\n",
    "        matches.extend(fulltext_matches_result)\n",
    "        matches.extend(single_matches_result)\n",
    "        return matches\n",
    "    \n",
    "    def get_suggestions(self, text):\n",
    "#         with Timer('Build query'):\n",
    "        q = self._build_query(text)\n",
    "        \n",
    "#         with Timer('Select matches'):\n",
    "        matches = self._select_matches(q)\n",
    "        return {\n",
    "            'text': text,\n",
    "            'matches': matches\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def pretty_print(entry):\n",
    "        lines = []\n",
    "        \n",
    "        matches = entry['matches']\n",
    "        query = entry['text']\n",
    "        \n",
    "        lines.append(f'Query: {query}\\n')\n",
    "        \n",
    "        for match in matches:\n",
    "            if match['name']:\n",
    "                lines.append(match['name'])\n",
    "            else:\n",
    "                lines.append('*no name*')\n",
    "\n",
    "            if match['description']:\n",
    "                desc = match['description']\n",
    "                lines.append(f'({desc})')\n",
    "\n",
    "            qid = match['qid']\n",
    "            lines.append(f'Link: https://www.wikidata.org/wiki/{qid}\\n')\n",
    "        \n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrph = Morpher()\n",
    "mtc = Matcher(es_instance=es)\n",
    "suggester = Suggester(matcher=mtc, tokenizer=tokenizer, morpher=mrph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Какой титул носил белогвардейский генерал Петр Николаевич Врангель?\n",
      "\n",
      "Пётр Николаевич Врангель\n",
      "(русский военачальник, один из главных руководителей Белого движения в годы Гражданской войны)\n",
      "Link: https://www.wikidata.org/wiki/Q108260\n",
      "\n",
      "Врангель, Николай Николаевич\n",
      "Link: https://www.wikidata.org/wiki/Q4126988\n",
      "\n",
      "Пётр Николаевич\n",
      "(Великий князь)\n",
      "Link: https://www.wikidata.org/wiki/Q446676\n",
      "\n",
      "Белянин, Пётр Николаевич\n",
      "Link: https://www.wikidata.org/wiki/Q15064452\n",
      "\n",
      "титул\n",
      "(почётное звание)\n",
      "Link: https://www.wikidata.org/wiki/Q216353\n",
      "\n",
      "Врангель (Находка)\n",
      "Link: https://www.wikidata.org/wiki/Q4126992\n",
      "\n",
      "Врангели\n",
      "Link: https://www.wikidata.org/wiki/Q689100\n",
      "\n",
      "Врангель\n",
      "(город в штате Аляска, США)\n",
      "Link: https://www.wikidata.org/wiki/Q43983\n",
      "\n",
      "почётное звание\n",
      "(звание, присваиваемое как награда)\n",
      "Link: https://www.wikidata.org/wiki/Q3320743\n",
      "\n",
      "Александр II\n",
      "(Император Всероссийский (1855—1881))\n",
      "Link: https://www.wikidata.org/wiki/Q83171\n",
      "\n",
      "Апостол Пётр\n",
      "(один из двенадцати апостолов (учеников) Иисуса Христа)\n",
      "Link: https://www.wikidata.org/wiki/Q33923\n",
      "\n",
      "Остров Врангеля\n",
      "Link: https://www.wikidata.org/wiki/Q106594\n",
      "\n",
      "Make suggestions executed in 8.422 s.\n"
     ]
    }
   ],
   "source": [
    "with Timer('Make suggestions'):\n",
    "    suggestions = suggester.get_suggestions('Какой титул носил белогвардейский генерал Петр Николаевич Врангель?')\n",
    "    print(Suggester.pretty_print(suggestions))\n",
    "#     print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc3bbc4c3cb45c69eb93303dd5c097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delim = '######'\n",
    "with open('quiz_test.txt', 'r') as inf, open('quiz_test_entities.txt', 'w') as ouf:\n",
    "    for i, line in enumerate(tqdm(inf)):\n",
    "        if i % 2 == 1:\n",
    "            continue\n",
    "        question = line.strip('\\n')\n",
    "        print(delim, file=ouf)\n",
    "        print(f'{i // 2 + 1}. ', end='', file=ouf)\n",
    "        suggestions = suggester.get_suggestions(question)\n",
    "        print(Suggester.pretty_print(suggestions), file=ouf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'макс',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 6,\n",
       "   'type': 'word',\n",
       "   'position': 0},\n",
       "  {'token': 'галкин',\n",
       "   'start_offset': 7,\n",
       "   'end_offset': 13,\n",
       "   'type': 'word',\n",
       "   'position': 1}]}"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = {\n",
    "    'tokenizer': 'whitespace',\n",
    "    'filter': ['lowercase', 'snow_stem'],\n",
    "#     'text': 'Эскиз декорации III го акт оперы А Спендиарова  Алмаст '\n",
    "    'text': 'Максим Галкин'\n",
    "}\n",
    "\n",
    "es.indices.analyze(index='label_single', body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'максим',\n",
       "    'wt': 0.8066484349,\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Максима'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'галкин',\n",
       "    'wt': 0.7169838121,\n",
       "    'gr': 'S,фам,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Галкина'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.analyze(tokenizer('Кого сыграл Жан Рено в фильме \"Ее звали Никита\"?'))\n",
    "mystem.analyze('Максима Галкина')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos_tags = [\n",
    "    'ADVPRO',\n",
    "    'APRO',\n",
    "    'CONJ',\n",
    "    'INTJ',\n",
    "    'PART',\n",
    "    'PR',\n",
    "    'SPRO',\n",
    "    'V',\n",
    "    'ADV'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'в',\n",
       "    'wt': 8.212235587e-06,\n",
       "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
       "  'text': 'в'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.analyze(tokenizer('в'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "an = vec.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['бой', 'августе', '1702', 'года', 'lol']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an('Бой в августе 1702 года (lol)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ArithmeticErro'.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-429-118b2869eb81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "d = {20: None, 3: 5}\n",
    "sum(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resp = requests.get('https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids=Q3656114|Q17&sitefilter=ruwiki&props=sitelinks')\n",
    "# resp = requests.get('https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids=Q42|Q17&languages=ru&props=labels|descriptions')\n",
    "# resp = requests.get('https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ru.wikipedia/all-access/all-agents/Обама,_Барак/monthly/2019010100/2019013100')\n",
    "resp = requests.get('https://ru.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&pvipdays=20&titles=(2208)_Пушкин')\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': '',\n",
       " 'query': {'normalized': [{'from': '(2208)_Пушкин', 'to': '(2208) Пушкин'}],\n",
       "  'pages': {'4326905': {'pageid': 4326905,\n",
       "    'ns': 0,\n",
       "    'title': '(2208) Пушкин',\n",
       "    'pageviews': {'2019-08-02': 0,\n",
       "     '2019-08-03': 0,\n",
       "     '2019-08-04': 0,\n",
       "     '2019-08-05': 2,\n",
       "     '2019-08-06': 0,\n",
       "     '2019-08-07': 0,\n",
       "     '2019-08-08': 2,\n",
       "     '2019-08-09': 1,\n",
       "     '2019-08-10': 0,\n",
       "     '2019-08-11': 1,\n",
       "     '2019-08-12': 3,\n",
       "     '2019-08-13': 1,\n",
       "     '2019-08-14': 2,\n",
       "     '2019-08-15': 2,\n",
       "     '2019-08-16': 1,\n",
       "     '2019-08-17': 2,\n",
       "     '2019-08-18': 0,\n",
       "     '2019-08-19': 2,\n",
       "     '2019-08-20': 1,\n",
       "     '2019-08-21': None}}}}}"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resp.json()['query']['pages'].values()\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499999500000\n",
      "Lalka sasatb executed in 0.149 s.\n"
     ]
    }
   ],
   "source": [
    "with Timer('Lalka sasatb'):\n",
    "    s = 0\n",
    "    for i in range(1000000):\n",
    "        s += i\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'tqdm_notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-625-fa19ec7d6fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'tqdm_notebook'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kbqa]",
   "language": "python",
   "name": "conda-env-kbqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
