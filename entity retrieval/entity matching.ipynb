{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:55:10.019384Z",
     "start_time": "2019-09-09T14:55:07.391978Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb6c75e046e49eba90519b837e768a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('labels_new_v3.txt', 'r') as inf:\n",
    "    labels = defaultdict(list)\n",
    "    for line in tqdm(inf):\n",
    "        qid, label = line.strip('\\n').split(':', 1)\n",
    "        labels[qid].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc895e5329934bc2b6d287fd7234aaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4114595), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in tqdm(labels.items()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:55:12.904824Z",
     "start_time": "2019-09-09T14:55:12.895352Z"
    }
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, job_name=None):\n",
    "        self.job_name = job_name\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time()\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        end_time = time()\n",
    "        passed_time = end_time - self.start_time\n",
    "        line = f'{self.job_name} executed in {passed_time:.3f} s.' if self.job_name else f'{passed_time:.3f} s.'\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply analyzer to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:55:17.388903Z",
     "start_time": "2019-09-09T14:55:17.380564Z"
    }
   },
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "simple_tokenizer = CountVectorizer(lowercase=False, token_pattern='\\w+').build_analyzer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return(' '.join(simple_tokenizer(text)))\n",
    "\n",
    "def analyzer(text):\n",
    "    return(' '.join(simple_analyzer(''.join(mystem.lemmatize(text)))))\n",
    "\n",
    "def analyzer2(text):\n",
    "    return(''.join(mystem.lemmatize(' '.join(simple_analyzer(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996b5b05ce9b4acbbee17588d1ded978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with open('labels_new_v3_test.txt', 'r') as inf, open('labels_mystem_test.txt', 'w') as ouf:\n",
    "#     for line in tqdm(inf):\n",
    "#         qid, label = line.strip('\\n').split(':', 1)\n",
    "#         ouf.write(qid + ':' + analyzer(label) + '\\n')\n",
    " \n",
    "# create file with only tokenized labels\n",
    "with open('labels_raw.txt', 'r') as inf, open('labels_token.txt', 'w') as ouf:\n",
    "    for line in tqdm(inf):\n",
    "        qid, label = line.strip('\\n').split(':', 1)\n",
    "        ouf.write(qid + ':' + tokenizer(label) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create elastic search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:55:35.439410Z",
     "start_time": "2019-09-09T14:55:35.432229Z"
    }
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def create_es_action(key, data, index):\n",
    "    es_document = {'qid': key, 'label': data}\n",
    "    es_action = {\n",
    "        \"_index\": index,\n",
    "        \"_source\": es_document\n",
    "    }\n",
    "    return es_action\n",
    "\n",
    "documents_file = 'labels_token.txt'\n",
    "\n",
    "def label_list_documents_generator():\n",
    "    with open(documents_file, 'r') as file:\n",
    "        prev_qid = None\n",
    "        cur_label_list = []\n",
    "        for line in tqdm(file):\n",
    "            qid, label = line.split(':', 1)\n",
    "            if qid == prev_qid:\n",
    "                cur_label_list.append(label.strip('\\n'))\n",
    "            else:\n",
    "                if len(cur_label_list) > 0:\n",
    "                    yield create_es_action(prev_qid, cur_label_list, 'label_list')\n",
    "\n",
    "                prev_qid = qid\n",
    "                cur_label_list = [label.strip('\\n')]\n",
    "        \n",
    "        if len(cur_label_list) > 0:\n",
    "            yield create_es_action(prev_qid, cur_label_list, 'label_list')\n",
    "            \n",
    "def label_single_documents_generator():\n",
    "    with open(documents_file, 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            qid, label = line.split(':', 1)\n",
    "            yield create_es_action(qid, label.strip('\\n'), 'label_single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'label_single'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"text\",\n",
    "                'analyzer': 'snowball_stemmer'\n",
    "#                 \"fields\": {\n",
    "#                     \"shingle\": {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"analyzer\": \"shingle_analyzer\"\n",
    "#                     }\n",
    "#                 }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\" : {\n",
    "            \"analyzer\" : {\n",
    "                \"snowball_stemmer\" : {\n",
    "                    \"tokenizer\" : \"whitespace\",\n",
    "                    \"filter\" : ['lowercase', \"snow_stem\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"snow_stem\" : {\n",
    "                    \"type\" : \"snowball\",\n",
    "                    \"language\" : \"Russian\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "#         \"analysis\": {\n",
    "#             \"analyzer\": {\n",
    "#                 \"shingle_analyzer\": {\n",
    "#                     \"tokenizer\": \"standard\",\n",
    "#                     \"filter\": [\n",
    "#                         \"custom_shingle\"\n",
    "#                     ]\n",
    "#                 }\n",
    "#             },\n",
    "#             \"filter\": {\n",
    "#                 \"custom_shingle\": {\n",
    "#                     \"type\": \"shingle\",\n",
    "#                     \"min_shingle_size\": \"2\",\n",
    "#                     \"max_shingle_size\": \"3\"\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.delete(index='label_list')\n",
    "es.indices.create(index='label_list', body=settings)\n",
    "\n",
    "es.indices.delete(index='label_single')\n",
    "es.indices.create(index='label_single', body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccc1f6d1a3445efa4d601983be8d65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ok, result in parallel_bulk(es, label_single_documents_generator(), queue_size=8, thread_count=8, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic query builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:56:06.749180Z",
     "start_time": "2019-09-09T14:56:06.743494Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('russian_stopwords.txt', 'r') as inf:\n",
    "    stopwords = []\n",
    "    for line in inf:\n",
    "        stopwords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:56:07.517203Z",
     "start_time": "2019-09-09T14:56:07.510815Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_pos_tags = [\n",
    "    'ADVPRO',\n",
    "    'APRO',\n",
    "    'CONJ',\n",
    "    'INTJ',\n",
    "    'PART',\n",
    "    'PR',\n",
    "    'SPRO',\n",
    "    'V',\n",
    "    'ADV'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:56:10.493000Z",
     "start_time": "2019-09-09T14:56:10.485374Z"
    }
   },
   "outputs": [],
   "source": [
    "# class TokenFilter:\n",
    "#     def __init__(self, stopwords, stop_pos_tags):\n",
    "#         self.stopwords = set(stopwords)\n",
    "#         self.stop_pos_tags = set(stop_pos_tags)\n",
    "        \n",
    "#     def is_good(self, token):\n",
    "#         if self.has_first_capital(token):\n",
    "#             return True\n",
    "#         if self.pass_stopwords(token) and \\\n",
    "#            self.pass_length(token):\n",
    "#             return True\n",
    "#         return False\n",
    "        \n",
    "#     def pass_stopwords(self, token):\n",
    "#         if token in self.stopwords:\n",
    "#             return False\n",
    "#         return True\n",
    "    \n",
    "#     def pass_length(self, token):\n",
    "#         if len(token) < 2:\n",
    "#             return False\n",
    "#         return True\n",
    "    \n",
    "#     def has_first_capital(self, token):\n",
    "#         if token[0].isupper():\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "class Morpher:\n",
    "    def __init__(self):\n",
    "        self.stop_pos_tags = set([\n",
    "            'ADVPRO',\n",
    "            'APRO',\n",
    "            'CONJ',\n",
    "            'INTJ',\n",
    "            'PART',\n",
    "            'PR',\n",
    "            'SPRO',\n",
    "            'V',\n",
    "            'ADV'\n",
    "        ])\n",
    "        self.mystem = Mystem()\n",
    "        \n",
    "    def analyze(self, text):\n",
    "        return self.mystem.analyze(text)\n",
    "    \n",
    "    def approve_tag(self, tag):\n",
    "        if tag in self.stop_pos_tags:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:56:12.324666Z",
     "start_time": "2019-09-09T14:56:12.295210Z"
    }
   },
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, text, tokenizer, morpher):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.morpher = morpher\n",
    "        self.get_tokens()\n",
    "        self.get_filtered_tokens()\n",
    "        self.get_token_ngrams()\n",
    "        self.get_capital_pairs()\n",
    "        \n",
    "    def get_tokens(self):\n",
    "        self.tokens = self.tokenizer(self.text).split()\n",
    "        return self.tokens\n",
    "    \n",
    "    def get_filtered_tokens(self):\n",
    "        self.filtered_tokens = []\n",
    "        analysis = self.morpher.analyze(' '.join(self.tokens))\n",
    "        for i, ta in enumerate(analysis):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if 'analysis' in ta:\n",
    "                if ta['text'][0].isupper() and len(ta['text']) > 1:\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "                    continue\n",
    "                if not ta['analysis']:\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "                    continue\n",
    "                pos_tag = ta['analysis'][0]['gr'].split(',', 1)[0].split('=', 1)[0]\n",
    "                if self.morpher.approve_tag(pos_tag):\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "        return self.filtered_tokens\n",
    "    \n",
    "    def get_token_ngrams(self, n=3):\n",
    "        self.token_ngrams = []\n",
    "        for i in range(len(self.tokens) - (n - 1)):\n",
    "            self.token_ngrams.append(' '.join(self.tokens[i:(i + n)]))\n",
    "        return self.token_ngrams\n",
    "    \n",
    "    def get_capital_pairs(self):\n",
    "        self.capital_pairs = []\n",
    "        for i in range(1, len(self.tokens) - 1):\n",
    "            if self.tokens[i][0].isupper() and \\\n",
    "               len(self.tokens[i]) > 1 and \\\n",
    "               self.tokens[i + 1][0].isupper() and \\\n",
    "               len(self.tokens[i + 1]) > 1:\n",
    "                self.capital_pairs.append(' '.join(self.tokens[i:(i + 2)]))\n",
    "        return self.capital_pairs\n",
    "    \n",
    "    def build_match_query(self, query, fuzziness='AUTO'):\n",
    "        return  { \n",
    "                    'match': {\n",
    "                        'label': {\n",
    "                            'query': query,\n",
    "                            \"fuzziness\": fuzziness,\n",
    "                            \"prefix_length\": 1,\n",
    "                            'fuzzy_transpositions': False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    def build_phrase_query(self, query):\n",
    "        return  { \n",
    "                    'match_phrase': {\n",
    "                        'label': query\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    def get_phrase_queries(self):\n",
    "        qs = []\n",
    "        for ng in self.token_ngrams:\n",
    "            qs.append(self.build_phrase_query(ng))\n",
    "        for cp in self.capital_pairs:\n",
    "            qs.append(self.build_phrase_query(cp))\n",
    "        return qs\n",
    "    \n",
    "    def get_fulltext_queries(self):\n",
    "        return [self.build_match_query(' '.join(self.filtered_tokens))]\n",
    "    \n",
    "    def get_single_capital_queries(self):\n",
    "        qs = []\n",
    "        for token in self.filtered_tokens:\n",
    "            if token[0].isupper() and len(token) > 2:\n",
    "                qs.append(self.build_phrase_query(token))\n",
    "        return qs\n",
    "    \n",
    "    def get_partial_queries(self):\n",
    "        qs = []\n",
    "        n = len(self.filtered_tokens)\n",
    "        for i in range(0, n, 2):\n",
    "            subtext = ' '.join(self.filtered_tokens[i:(min(i + 3, n))])\n",
    "            qs.append(self.build_match_query(subtext, fuzziness=0))\n",
    "        return qs\n",
    "        \n",
    "    def build_es_query(self, queries):\n",
    "        return  {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'should': queries\n",
    "                        }\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T15:12:42.807600Z",
     "start_time": "2019-09-09T15:12:42.797086Z"
    }
   },
   "outputs": [],
   "source": [
    "class QueryTarget(Query):\n",
    "    def get_filtered_tokens(self):\n",
    "        self.filtered_tokens = []\n",
    "        analysis = self.morpher.analyze(' '.join(self.tokens))\n",
    "        for ta in analysis:\n",
    "            if 'analysis' in ta:\n",
    "                if ta['text'][0].isupper() and len(ta['text']) > 1:\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "                    continue\n",
    "                if not ta['analysis']:\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "                    continue\n",
    "                pos_tag = ta['analysis'][0]['gr'].split(',', 1)[0].split('=', 1)[0]\n",
    "                if self.morpher.approve_tag(pos_tag):\n",
    "                    self.filtered_tokens.append(ta['text'])\n",
    "        return self.filtered_tokens\n",
    "    \n",
    "    def get_token_ngrams(self, n=2):\n",
    "        self.token_ngrams = [' '.join(self.tokens)]\n",
    "        for i in range(len(self.tokens) - (n - 1)):\n",
    "            self.token_ngrams.append(' '.join(self.tokens[i:(i + n)]))\n",
    "        return self.token_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T15:14:25.422373Z",
     "start_time": "2019-09-09T15:14:24.097391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.314 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'match_phrase': {'label': 'О какой битве'}},\n",
       " {'match_phrase': {'label': 'какой битве в'}},\n",
       " {'match_phrase': {'label': 'битве в российской'}},\n",
       " {'match_phrase': {'label': 'в российской истории'}},\n",
       " {'match_phrase': {'label': 'российской истории говорил'}},\n",
       " {'match_phrase': {'label': 'истории говорил русский'}},\n",
       " {'match_phrase': {'label': 'говорил русский поэт'}},\n",
       " {'match_phrase': {'label': 'русский поэт А'}},\n",
       " {'match_phrase': {'label': 'поэт А Пушкин'}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrph = Morpher()\n",
    "with Timer():\n",
    "    q = Query('О какой битве в российской истории говорил русский поэт А . Пушкин?', tokenizer=tokenizer, morpher=mrph)\n",
    "q.get_phrase_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:56:30.707583Z",
     "start_time": "2019-09-09T14:56:30.699851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'bool': {'should': [{'match_phrase': {'label': 'О какой битве'}},\n",
       "    {'match_phrase': {'label': 'какой битве в'}},\n",
       "    {'match_phrase': {'label': 'битве в российской'}},\n",
       "    {'match_phrase': {'label': 'в российской истории'}},\n",
       "    {'match_phrase': {'label': 'российской истории говорил'}},\n",
       "    {'match_phrase': {'label': 'истории говорил русский'}},\n",
       "    {'match_phrase': {'label': 'говорил русский поэт'}},\n",
       "    {'match_phrase': {'label': 'русский поэт А'}},\n",
       "    {'match_phrase': {'label': 'поэт А Пушкин'}}]}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.build_es_query(q.get_phrase_queries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:43:19.678564Z",
     "start_time": "2019-09-09T18:43:19.639186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Чехов']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': {'value': 1305, 'relation': 'eq'},\n",
       " 'max_score': 12.181487,\n",
       " 'hits': [{'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'nf_wpmwBg_1FN09Hwjtz',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q5201908', 'label': 'Чехи'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '_BnypmwBg_1FN09HAm46',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q16723335', 'label': 'Чехи'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'Nf3wpmwBg_1FN09HqS32',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q4515194', 'label': 'Чехи'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'O9rupmwBg_1FN09H9XQR',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q39193', 'label': 'Чехия'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'C9rupmwBg_1FN09H9Ymq',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q42585', 'label': 'Чехия'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'odnupmwBg_1FN09H6bOk',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q213', 'label': 'Чехия'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'A93vpmwBg_1FN09HHb9L',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q198087', 'label': 'Чехов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '993vpmwBg_1FN09HFzV4',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q170217', 'label': 'чехи'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '9d3vpmwBg_1FN09HFzV4',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q170217', 'label': 'чех'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'dt_vpmwBg_1FN09HOt9g',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q338056', 'label': 'Чех'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'Mv3wpmwBg_1FN09HqS32',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q4515190', 'label': 'Чехи'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '4uXvpmwBg_1FN09HlKvF',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q912790', 'label': 'Чехов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'Tv3wpmwBg_1FN09HqS32',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q4515211', 'label': 'Чехов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'g_TwpmwBg_1FN09HQUT1',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q3696596', 'label': 'Чехов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '9vTwpmwBg_1FN09HRHZM',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q3740394', 'label': 'Чехов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'l-3vpmwBg_1FN09H7R9h',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q2162698', 'label': 'Чехия'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '7PPwpmwBg_1FN09HO6p-',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q3496079', 'label': 'Чехия'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': '5ufvpmwBg_1FN09Hpizj',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q1080079', 'label': 'Чех'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'x-rvpmwBg_1FN09HxwGO',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q1631586', 'label': 'Чехов'}},\n",
       "  {'_index': 'label_single',\n",
       "   '_type': '_doc',\n",
       "   '_id': 'juvvpmwBg_1FN09H3-B-',\n",
       "   '_score': 12.181487,\n",
       "   '_source': {'qid': 'Q1991965', 'label': 'Чехия'}}]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = tokenizer('Кого сыграл Жан Рено в фильме \"Ее звали Никита\"?')\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         \"fuzzy\" : {\n",
    "#             \"labels\" : {\n",
    "#                 \"value\": \"домашняя кошка\",\n",
    "#                 \"fuzziness\": 2,\n",
    "#                 \"prefix_length\": 0,\n",
    "#                 \"max_expansions\": 100,\n",
    "#                 'transpositions': False\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         'match': {\n",
    "#             'label': {\n",
    "#                 'query': query,\n",
    "#                 \"fuzziness\": 'AUTO',\n",
    "#                 \"prefix_length\": 1,\n",
    "#                 'fuzzy_transpositions': False\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         'match_phrase': {\n",
    "#             'label': query\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "doc = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'should': [\n",
    "#                 {\n",
    "#                     'match_phrase': {\n",
    "#                         'label': 'Жан Рено'\n",
    "#                     }\n",
    "#                 },\n",
    "#                 {\n",
    "#                     'match_phrase': {\n",
    "#                         'label': 'Ее звали Никита'\n",
    "#                     }\n",
    "#                 },\n",
    "                { \n",
    "                    'match': {\n",
    "                        'label': {\n",
    "                            'query': 'На Земле',\n",
    "                            \"fuzziness\": 'AUTO',\n",
    "                            \"prefix_length\": 1,\n",
    "                            'fuzzy_transpositions': False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# doc = {\n",
    "#     'query': {\n",
    "#         'match_all': {}\n",
    "#     }\n",
    "# }\n",
    "q = QueryTarget('Чехов', tokenizer=tokenizer, morpher=mrph)\n",
    "print(q.filtered_tokens)\n",
    "doc = q.build_es_query(q.get_fulltext_queries())\n",
    "\n",
    "res = es.search(index='label_single', body=doc, size=20)\n",
    "res['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T01:21:15.183506Z",
     "start_time": "2019-09-21T01:21:14.458147Z"
    }
   },
   "outputs": [],
   "source": [
    "class Matcher:\n",
    "    def __init__(self, es_instance):\n",
    "        self.es = es_instance\n",
    "        \n",
    "    def _smooth_score(self, score, d=5):\n",
    "        score = int(score)\n",
    "        while score % d != 0:\n",
    "            score += 1\n",
    "        return score\n",
    "    \n",
    "    def _parse_id(self, qid):\n",
    "        return int(qid[1:])\n",
    "    \n",
    "    def _sorting_key(self, match):\n",
    "        score = self._smooth_score(match['score'])\n",
    "        qid = self._parse_id(match['qid'])\n",
    "        return score, -qid\n",
    "    \n",
    "    def get_names_and_descriptions(self, qids):\n",
    "        if not qids:\n",
    "            return {}\n",
    "        qids_list = '|'.join(qids)\n",
    "        wikiapi_query = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={qids_list}&languages=ru&props=labels|descriptions'\n",
    "        resp = requests.get(wikiapi_query).json()\n",
    "        result = {}\n",
    "        for qid in qids:\n",
    "            cur_data = resp['entities'][qid]\n",
    "            name = None\n",
    "            if 'labels' in cur_data:\n",
    "                if 'ru' in cur_data['labels']:\n",
    "                    name = cur_data['labels']['ru']['value']\n",
    "            description = None\n",
    "            if 'descriptions' in cur_data:\n",
    "                if 'ru' in cur_data['descriptions']:\n",
    "                    description = cur_data['descriptions']['ru']['value']\n",
    "            result[qid] = {'name': name, 'description': description}\n",
    "        return result\n",
    "    \n",
    "    def get_wikipedia_pageviews(self, titles_dict):\n",
    "        if not titles_dict:\n",
    "            return\n",
    "        titles_list = list(map(lambda s: s.replace(' ', '_'), titles_dict.keys()))\n",
    "        for i in range(len(titles_list)):\n",
    "            titles = '|'.join(titles_list)\n",
    "            wikiapi_query = f'https://ru.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&pvipdays=30&titles={titles}'\n",
    "            resp = requests.get(wikiapi_query).json()\n",
    "            if 'batchcomplete' in resp:\n",
    "                break\n",
    "#         print(titles_list)\n",
    "#         print(resp)\n",
    "        pages_data = (resp['query']['pages'])\n",
    "#         print(len(titles_list))\n",
    "#         print(len(pages_data.values()))\n",
    "        \n",
    "        for entry in pages_data.values():\n",
    "            view_stats = entry['pageviews']\n",
    "            views = sum(filter(None, view_stats.values()))\n",
    "            cur_title = entry['title']\n",
    "            titles_dict[cur_title] = views\n",
    "    \n",
    "    def get_wikipedia_pages(self, qids):\n",
    "        if not qids:\n",
    "            return {}\n",
    "        qids_list = '|'.join(qids)\n",
    "        wikiapi_query = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={qids_list}&sitefilter=ruwiki&props=sitelinks'\n",
    "        resp = requests.get(wikiapi_query).json()\n",
    "        result = {}\n",
    "        views = {}\n",
    "        for qid in qids:\n",
    "            cur_data = resp['entities'][qid]\n",
    "            wiki_title = None\n",
    "            if 'sitelinks' in cur_data and 'ruwiki' in cur_data['sitelinks'] and 'title' in cur_data['sitelinks']['ruwiki']:\n",
    "                wiki_title = cur_data['sitelinks']['ruwiki']['title']\n",
    "                views[wiki_title] = 0\n",
    "            result[qid] = {'ruwiki': wiki_title}\n",
    "            \n",
    "        self.get_wikipedia_pageviews(views)\n",
    "        for qid in qids:\n",
    "            cur_title = result[qid]['ruwiki']\n",
    "            if cur_title is not None:\n",
    "                result[qid]['views'] = views[cur_title]\n",
    "            else:\n",
    "                result[qid]['views'] = 0\n",
    "        return result\n",
    "    \n",
    "#     def get_wikipedia_pageviews(self, wiki_title):\n",
    "#         wiki_title = wiki_title.replace(' ', '_')\n",
    "#         wikiapi_query = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ru.wikipedia/all-access/all-agents/{wiki_title}/monthly/2019010100/2019013100'\n",
    "#         resp = requests.get(wikiapi_query).json()\n",
    "#         return resp['items'][0]['views']\n",
    "\n",
    "    def _apply_ranking(self, matches, relative_rate=0.9):\n",
    "        if len(matches) == 0:\n",
    "            return matches\n",
    "        \n",
    "        matches = sorted(matches, key=lambda m: m['score'], reverse=True)      \n",
    "        \n",
    "        cur_max_pos = 0\n",
    "        cur_pos = 1\n",
    "        while cur_pos < len(matches):\n",
    "            cur_max_score = matches[cur_max_pos]['score']\n",
    "            while cur_pos < len(matches) and matches[cur_pos]['score'] >= relative_rate * matches[cur_pos - 1]['score']:\n",
    "                cur_pos += 1\n",
    "            matches[cur_max_pos:cur_pos] = sorted(matches[cur_max_pos:cur_pos], key=lambda m: m['views'], reverse=True)\n",
    "            cur_max_pos = cur_pos\n",
    "            cur_pos = cur_max_pos + 1\n",
    "            \n",
    "        return matches\n",
    "        \n",
    "    def get_query_matches(self, query, n_matches=30):\n",
    "        es_result = es.search(index='label_single', body=query, size=n_matches)['hits']\n",
    "        matches_dict = {}\n",
    "        for entry in es_result['hits']:\n",
    "            qid = entry['_source']['qid']\n",
    "            if qid not in matches_dict:\n",
    "                matches_dict[qid] = entry['_source']\n",
    "                matches_dict[qid]['score'] = entry['_score']\n",
    "        \n",
    "        names_and_descriptions = self.get_names_and_descriptions(matches_dict.keys())\n",
    "        for qid, nds in names_and_descriptions.items():\n",
    "            matches_dict[qid].update(nds)\n",
    "            \n",
    "        wikipedia_pages = self.get_wikipedia_pages(matches_dict.keys())\n",
    "        for qid, wps in wikipedia_pages.items():\n",
    "            matches_dict[qid].update(wps)\n",
    "            \n",
    "#         for qid in tqdm(matches_dict):\n",
    "#             match = matches_dict[qid]\n",
    "#             match['views'] = 0\n",
    "#             if match['ruwiki'] is not None:\n",
    "#                 match['views'] = self.get_wikipedia_pageviews(match['ruwiki'])\n",
    "            \n",
    "#         matches = sorted(matches_dict.values(), key=self._sorting_key, reverse=True)\n",
    "        matches = self._apply_ranking(list(matches_dict.values()))\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtc = Matcher(es_instance=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'bool': {'should': [{'match_phrase': {'label': 'Какое профессиональное прозвище'}},\n",
       "    {'match_phrase': {'label': 'профессиональное прозвище у'}},\n",
       "    {'match_phrase': {'label': 'прозвище у Максима'}},\n",
       "    {'match_phrase': {'label': 'у Максима Галкина'}},\n",
       "    {'match_phrase': {'label': 'Максима Галкина'}}]}}}"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.build_es_query(q.get_single_capital_queries()[0])\n",
    "q.build_es_query(q.get_phrase_queries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qid': 'Q32360836',\n",
       "  'label': 'Кто хочет стать Максимом Галкиным',\n",
       "  'score': 12.354108,\n",
       "  'name': 'Кто хочет стать Максимом Галкиным?',\n",
       "  'description': None,\n",
       "  'ruwiki': 'Кто хочет стать Максимом Галкиным?',\n",
       "  'views': 324},\n",
       " {'qid': 'Q245445',\n",
       "  'label': 'Кто хочет стать Максимом Галкиным',\n",
       "  'score': 12.354108,\n",
       "  'name': 'Кто хочет стать Максимом Галкиным?',\n",
       "  'description': None,\n",
       "  'ruwiki': None,\n",
       "  'views': 0}]"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mtc.get_query_matches(q.build_es_query(q.get_fulltext_queries()))\n",
    "# mtc.get_query_matches(q.build_es_query(q.get_single_capital_queries()[0]))\n",
    "mtc.get_query_matches(q.build_es_query(q.get_phrase_queries()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T15:01:30.890298Z",
     "start_time": "2019-09-09T15:01:30.863755Z"
    }
   },
   "outputs": [],
   "source": [
    "class Suggester:\n",
    "    def __init__(self, matcher, tokenizer, morpher):\n",
    "        self.matcher = matcher\n",
    "        self.tokenizer = tokenizer\n",
    "        self.morpher = morpher\n",
    "        \n",
    "    def _build_query(self, text):\n",
    "        return Query(text=text, tokenizer=self.tokenizer, morpher=self.morpher)\n",
    "    \n",
    "    def _get_matches(self, q, query_type):\n",
    "        if query_type == 'fulltext':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_fulltext_queries()), n_matches=20)\n",
    "        if query_type == 'phrase':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_phrase_queries()), n_matches=8)\n",
    "        if query_type == 'single':\n",
    "            single_queries = q.get_single_capital_queries()\n",
    "            return [self.matcher.get_query_matches(q.build_es_query(single_query), n_matches=10)\n",
    "                   for single_query in single_queries]\n",
    "        \n",
    "    def _select_matches(self, q, min_total=8, f=5, p=4, s=1):\n",
    "        matches = []\n",
    "        matches_qids = set()\n",
    "        \n",
    "#         print('# Phrase')\n",
    "        phrase_matches = self._get_matches(q, query_type='phrase')\n",
    "        if len(phrase_matches) > p:\n",
    "            phrase_matches = phrase_matches[:p]\n",
    "        for match in phrase_matches:\n",
    "            matches_qids.add(match['qid'])\n",
    "            match['source'] = 'phrase'\n",
    "#             print(match)\n",
    "        \n",
    "#         print('# Singles')\n",
    "        single_matches = self._get_matches(q, query_type='single')\n",
    "        single_matches_result = []\n",
    "        for single_match in single_matches:\n",
    "            if not single_match:\n",
    "                continue\n",
    "            top_match = single_match[0]\n",
    "            if top_match['qid'] not in matches_qids:\n",
    "                matches_qids.add(top_match['qid'])\n",
    "                top_match['source'] = 'single'\n",
    "                single_matches_result.append(top_match)\n",
    "#                 print(top_match)\n",
    "        single_matches_result = sorted(single_matches_result, key=lambda m: m['views'], reverse=True)\n",
    "        \n",
    "#         print('# Fulltext')\n",
    "        fulltext_matches = self._get_matches(q, query_type='fulltext')\n",
    "        fulltext_matches_result = []\n",
    "        f_cnt = 0\n",
    "        for match in fulltext_matches:\n",
    "            if match['qid'] not in matches_qids:\n",
    "                f_cnt += 1\n",
    "                matches_qids.add(match['qid'])\n",
    "                match['source'] = 'fulltext'\n",
    "                fulltext_matches_result.append(match)\n",
    "#                 print(match)\n",
    "            if len(matches_qids) >= min_total and f_cnt >= f:\n",
    "                break\n",
    "        fulltext_matches_result = sorted(fulltext_matches_result, key=lambda m: m['score'], reverse=True)\n",
    "                \n",
    "        matches.extend(phrase_matches)\n",
    "        matches.extend(fulltext_matches_result)\n",
    "        matches.extend(single_matches_result)\n",
    "        return matches\n",
    "    \n",
    "    def get_suggestions(self, text):\n",
    "#         with Timer('Build query'):\n",
    "        q = self._build_query(text)\n",
    "        \n",
    "#         with Timer('Select matches'):\n",
    "        matches = self._select_matches(q)\n",
    "        return {\n",
    "            'text': text,\n",
    "            'matches': matches\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def pretty_print(entry):\n",
    "        lines = []\n",
    "        \n",
    "        matches = entry['matches']\n",
    "        query = entry['text']\n",
    "        \n",
    "        lines.append(f'Query: {query}\\n')\n",
    "        \n",
    "        for match in matches:\n",
    "            if match['name']:\n",
    "                lines.append(match['name'])\n",
    "            else:\n",
    "                lines.append('*no name*')\n",
    "\n",
    "            if match['description']:\n",
    "                desc = match['description']\n",
    "                lines.append(f'({desc})')\n",
    "\n",
    "            qid = match['qid']\n",
    "            lines.append(f'Link: https://www.wikidata.org/wiki/{qid}\\n')\n",
    "        \n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T15:52:07.061741Z",
     "start_time": "2019-09-09T15:52:07.052301Z"
    }
   },
   "outputs": [],
   "source": [
    "class SuggesterTarget(Suggester):\n",
    "    def _build_query(self, text):\n",
    "        return QueryTarget(text=text, tokenizer=self.tokenizer, morpher=self.morpher)\n",
    "    \n",
    "    def _get_matches(self, q, query_type):\n",
    "        if query_type == 'fulltext':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_fulltext_queries()), n_matches=5)\n",
    "        if query_type == 'phrase':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_phrase_queries()), n_matches=10)\n",
    "        if query_type == 'single':\n",
    "            single_queries = q.get_single_capital_queries()\n",
    "            return [self.matcher.get_query_matches(q.build_es_query(single_query), n_matches=5)\n",
    "                   for single_query in single_queries]\n",
    "    \n",
    "    def _select_matches(self, q, min_total=5, f=3, p=4, s=1):\n",
    "        return super()._select_matches(q, min_total, f, p, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T01:21:57.152114Z",
     "start_time": "2019-09-21T01:21:57.147465Z"
    }
   },
   "outputs": [],
   "source": [
    "mrph = Morpher()\n",
    "mtc = Matcher(es_instance=es)\n",
    "suggester = SuggesterTarget(matcher=mtc, tokenizer=tokenizer, morpher=mrph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T15:52:12.731224Z",
     "start_time": "2019-09-09T15:52:08.412514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Toyota\n",
      "\n",
      "Toyota\n",
      "Link: https://www.wikidata.org/wiki/Q53268\n",
      "\n",
      "Тойота\n",
      "Link: https://www.wikidata.org/wiki/Q61979\n",
      "\n",
      "(3533) Тойота\n",
      "(астероид)\n",
      "Link: https://www.wikidata.org/wiki/Q918231\n",
      "\n",
      "Toyota Yaris\n",
      "Link: https://www.wikidata.org/wiki/Q6902966\n",
      "\n",
      "Toyota Picnic\n",
      "Link: https://www.wikidata.org/wiki/Q14607963\n",
      "\n",
      "{'text': 'Toyota', 'matches': [{'qid': 'Q53268', 'label': 'Toyota', 'score': 13.163293, 'name': 'Toyota', 'description': None, 'ruwiki': 'Toyota', 'views': 19776, 'source': 'phrase'}, {'qid': 'Q61979', 'label': 'Toyota', 'score': 13.163293, 'name': 'Тойота', 'description': None, 'ruwiki': 'Тойота (команда «Формулы-1»)', 'views': 338, 'source': 'phrase'}, {'qid': 'Q918231', 'label': 'Toyota', 'score': 13.163293, 'name': '(3533) Тойота', 'description': 'астероид', 'ruwiki': None, 'views': 0, 'source': 'phrase'}, {'qid': 'Q6902966', 'label': 'Toyota Yaris', 'score': 10.560182, 'name': 'Toyota Yaris', 'description': None, 'ruwiki': 'Toyota Yaris', 'views': 3934, 'source': 'phrase'}, {'qid': 'Q14607963', 'label': 'Toyota Picnic', 'score': 10.560182, 'name': 'Toyota Picnic', 'description': None, 'ruwiki': 'Toyota Picnic', 'views': 1277, 'source': 'fulltext'}]}\n",
      "Make suggestions executed in 4.311 s.\n"
     ]
    }
   ],
   "source": [
    "with Timer('Make suggestions'):\n",
    "    suggestions = suggester.get_suggestions('Toyota')\n",
    "    print(Suggester.pretty_print(suggestions))\n",
    "    print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get entites from questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8d9c266b614f5e9d39f00ac0a5812d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delim = '######'\n",
    "with open('matching/quiz_dataset_1_5000.txt', 'r') as dataset, \\\n",
    "     open('matching/quiz_entities_1_5000.txt', 'w') as pretty:\n",
    "    for i, line in enumerate(tqdm(dataset)):\n",
    "        if i % 2 == 1 or i < 4848:\n",
    "            continue\n",
    "        question = line.strip('\\n')\n",
    "        print(delim, file=pretty)\n",
    "        print(f'{i // 2 + 1}. ', end='', file=pretty)\n",
    "        suggestions = suggester.get_suggestions(question)\n",
    "        suggestions_data.append(suggestions)\n",
    "        with open('matching/quiz_entities_data_1_5000.json', 'w') as results:\n",
    "            json.dump(suggestions_data, results, indent=4, ensure_ascii=False)\n",
    "        print(Suggester.pretty_print(suggestions), file=pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get entites from answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T21:23:59.931726Z",
     "start_time": "2019-09-18T21:23:59.908173Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "suggestions_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T03:02:05.304268Z",
     "start_time": "2019-09-21T01:28:23.265981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d92b8fb8b4481eba2d941cbd5c84ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delim = '######'\n",
    "with open('matching/quiz_dataset_5001_10000.txt', 'r') as dataset, \\\n",
    "     open('matching/quiz_answer_entities_5001_10000.txt', 'a') as pretty:\n",
    "    for i, line in enumerate(tqdm(dataset)):\n",
    "        if i % 2 == 0 or i < 7122:\n",
    "            question = line.strip('\\n')\n",
    "            continue\n",
    "        answer = line.strip('\\n')\n",
    "        print(delim, file=pretty)\n",
    "        print(f'{i // 2 + 1}.', file=pretty)\n",
    "        print(f'Question: {question}', file=pretty)\n",
    "        suggestions = suggester.get_suggestions(answer)\n",
    "        suggestions['question'] = question\n",
    "        suggestions_data.append(suggestions)\n",
    "        with open('matching/quiz_answer_entities_data_5001_10000.json', 'w') as results:\n",
    "            json.dump(suggestions_data, results, indent=4, ensure_ascii=False)\n",
    "        print(Suggester.pretty_print(suggestions), file=pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T11:01:52.300353Z",
     "start_time": "2019-09-10T11:01:52.260810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suggestions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'В каком году была Хиросима и Нагасаки?',\n",
       " 'matches': [{'qid': 'Q488',\n",
       "   'label': 'атомные бомбардировки Хиросимы и Нагасаки',\n",
       "   'score': 17.550804,\n",
       "   'name': 'атомные бомбардировки Хиросимы и Нагасаки',\n",
       "   'description': 'бомбардировки атомными бомбами японских городов 6 и 9 августа 1945 года',\n",
       "   'ruwiki': 'Атомные бомбардировки Хиросимы и Нагасаки',\n",
       "   'views': 68446,\n",
       "   'source': 'phrase'},\n",
       "  {'qid': 'Q169376',\n",
       "   'label': 'Нагасаки',\n",
       "   'score': 12.959096,\n",
       "   'name': 'Нагасаки',\n",
       "   'description': 'префектура Японии',\n",
       "   'ruwiki': 'Нагасаки (префектура)',\n",
       "   'views': 335,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q617375',\n",
       "   'label': 'Хиросима',\n",
       "   'score': 12.252915,\n",
       "   'name': 'Хиросима',\n",
       "   'description': 'префектура Японии',\n",
       "   'ruwiki': 'Хиросима (префектура)',\n",
       "   'views': 371,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q11276735',\n",
       "   'label': 'Хиросима',\n",
       "   'score': 12.252915,\n",
       "   'name': 'Хиросима',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Хиросима (фильм, 1953)',\n",
       "   'views': 252,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q1141203',\n",
       "   'label': 'Хиросима',\n",
       "   'score': 12.252915,\n",
       "   'name': 'Хиросима',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Хиросима (княжество)',\n",
       "   'views': 89,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q1304426',\n",
       "   'label': 'Нагамаки',\n",
       "   'score': 11.107797,\n",
       "   'name': 'Нагамаки',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Нагамаки',\n",
       "   'views': 1304,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q34664',\n",
       "   'label': 'Хиросима Хиросима',\n",
       "   'score': 15.816826,\n",
       "   'name': 'Хиросима',\n",
       "   'description': 'город в Японии',\n",
       "   'ruwiki': 'Хиросима',\n",
       "   'views': 14342,\n",
       "   'source': 'single'},\n",
       "  {'qid': 'Q38234',\n",
       "   'label': 'Нагасаки Нагасаки',\n",
       "   'score': 14.573887,\n",
       "   'name': 'Нагасаки',\n",
       "   'description': 'город Японии с расширенным самоуправлением',\n",
       "   'ruwiki': 'Нагасаки',\n",
       "   'views': 7091,\n",
       "   'source': 'single'}]}"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestions_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = tokenizer('Какое прозвище носил король Англии Ричард I?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'какой',\n",
       "    'wt': 0.9940105847,\n",
       "    'gr': 'APRO=(вин,ед,сред|им,ед,сред)'}],\n",
       "  'text': 'Какое'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'прозвище',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,сред,неод=(пр,ед|вин,ед|им,ед)'}],\n",
       "  'text': 'прозвище'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'носить',\n",
       "    'wt': 1,\n",
       "    'gr': 'V,несов,пе=прош,ед,изъяв,муж'}],\n",
       "  'text': 'носил'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'король', 'wt': 1, 'gr': 'S,муж,од=им,ед'}],\n",
       "  'text': 'король'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'англия',\n",
       "    'wt': 1,\n",
       "    'gr': 'S,гео,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}],\n",
       "  'text': 'Англии'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'ричард',\n",
       "    'wt': 0.9724140581,\n",
       "    'gr': 'S,имя,муж,од=им,ед'}],\n",
       "  'text': 'Ричард'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'I'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrph.analyze(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'ричард',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 6,\n",
       "   'type': 'word',\n",
       "   'position': 0},\n",
       "  {'token': 'i',\n",
       "   'start_offset': 7,\n",
       "   'end_offset': 8,\n",
       "   'type': 'word',\n",
       "   'position': 1}]}"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = {\n",
    "    'tokenizer': 'whitespace',\n",
    "    'filter': ['lowercase', 'snow_stem'],\n",
    "#     'text': 'Эскиз декорации III го акт оперы А Спендиарова  Алмаст '\n",
    "    'text': 'Ричард I'\n",
    "}\n",
    "\n",
    "es.indices.analyze(index='label_single', body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'максим',\n",
       "    'wt': 0.8066484349,\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Максима'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'галкин',\n",
       "    'wt': 0.7169838121,\n",
       "    'gr': 'S,фам,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Галкина'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.analyze(tokenizer('Кого сыграл Жан Рено в фильме \"Ее звали Никита\"?'))\n",
    "mystem.analyze('Максима Галкина')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_pos_tags = [\n",
    "    'ADVPRO',\n",
    "    'APRO',\n",
    "    'CONJ',\n",
    "    'INTJ',\n",
    "    'PART',\n",
    "    'PR',\n",
    "    'SPRO',\n",
    "    'V',\n",
    "    'ADV'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resp = requests.get('https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids=Q3656114|Q17&sitefilter=ruwiki&props=sitelinks')\n",
    "resp = requests.get('https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids=Q3656114|Q17&languages=ru&props=labels|descriptions')\n",
    "# resp = requests.get('https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ru.wikipedia/all-access/all-agents/Обама,_Барак/monthly/2019010100/2019013100')\n",
    "# resp = requests.get('https://ru.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&pvipdays=20&titles=(2208)_Пушкин')\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'Q3656114': {'type': 'item',\n",
       "   'id': 'Q3656114',\n",
       "   'labels': {'ru': {'language': 'ru',\n",
       "     'value': 'История государства Российского'}},\n",
       "   'descriptions': {}},\n",
       "  'Q17': {'type': 'item',\n",
       "   'id': 'Q17',\n",
       "   'labels': {'ru': {'language': 'ru', 'value': 'Япония'}},\n",
       "   'descriptions': {'ru': {'language': 'ru',\n",
       "     'value': 'островное государство в Восточной Азии'}}}},\n",
       " 'success': 1}"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resp.json()['query']['pages'].values()\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 'label_single',\n",
       " 'shard': 0,\n",
       " 'primary': False,\n",
       " 'current_state': 'unassigned',\n",
       " 'unassigned_info': {'reason': 'INDEX_CREATED',\n",
       "  'at': '2019-08-18T22:52:42.694Z',\n",
       "  'last_allocation_status': 'no_attempt'},\n",
       " 'cluster_info': {'nodes': {'T3rpd-0gSEePSbG0hzcyMA': {'node_name': 'Air-Vladislav.Dlink',\n",
       "    'least_available': {'path': '/usr/local/var/lib/elasticsearch/nodes/0',\n",
       "     'total_bytes': 121123069952,\n",
       "     'used_bytes': 114139070464,\n",
       "     'free_bytes': 6983999488,\n",
       "     'free_disk_percent': 5.8,\n",
       "     'used_disk_percent': 94.2},\n",
       "    'most_available': {'path': '/usr/local/var/lib/elasticsearch/nodes/0',\n",
       "     'total_bytes': 121123069952,\n",
       "     'used_bytes': 114139070464,\n",
       "     'free_bytes': 6983999488,\n",
       "     'free_disk_percent': 5.8,\n",
       "     'used_disk_percent': 94.2}}},\n",
       "  'shard_sizes': {'[label_list][0][p]_bytes': 283,\n",
       "   '[label_single][0][p]_bytes': 443925562,\n",
       "   '[labels][0][p]_bytes': 53812},\n",
       "  'shard_paths': {'[labels][0], node[T3rpd-0gSEePSbG0hzcyMA], [P], s[STARTED], a[id=D_zsl-unSMqIhOLH9CXJ9g]': '/usr/local/var/lib/elasticsearch/nodes/0',\n",
       "   '[label_single][0], node[T3rpd-0gSEePSbG0hzcyMA], [P], s[STARTED], a[id=ePKf8_f5QKSs6c26P3uggA]': '/usr/local/var/lib/elasticsearch/nodes/0',\n",
       "   '[label_list][0], node[T3rpd-0gSEePSbG0hzcyMA], [P], s[STARTED], a[id=ugpfPj8NQDiJ_yLS4oG6bQ]': '/usr/local/var/lib/elasticsearch/nodes/0'}},\n",
       " 'can_allocate': 'no',\n",
       " 'allocate_explanation': 'cannot allocate because allocation is not permitted to any of the nodes',\n",
       " 'node_allocation_decisions': [{'node_id': 'T3rpd-0gSEePSbG0hzcyMA',\n",
       "   'node_name': 'Air-Vladislav.Dlink',\n",
       "   'transport_address': '127.0.0.1:9300',\n",
       "   'node_attributes': {'ml.machine_memory': '8589934592',\n",
       "    'xpack.installed': 'true',\n",
       "    'ml.max_open_jobs': '20'},\n",
       "   'node_decision': 'no',\n",
       "   'weight_ranking': 1,\n",
       "   'deciders': [{'decider': 'same_shard',\n",
       "     'decision': 'NO',\n",
       "     'explanation': 'the shard cannot be allocated to the same node on which a copy of the shard already exists [[label_single][0], node[T3rpd-0gSEePSbG0hzcyMA], [P], s[STARTED], a[id=ePKf8_f5QKSs6c26P3uggA]]'}]}]}"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.cluster.allocation_explain(include_disk_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T06:28:31.236189Z",
     "start_time": "2019-09-19T06:28:30.923401Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': ''}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiapi_query = f'https://ru.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&pvipdays=30&titles='\n",
    "resp = requests.get(wikiapi_query).json()\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kbqa]",
   "language": "python",
   "name": "conda-env-kbqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
